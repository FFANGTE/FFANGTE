{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import parser\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "import torchvision\n",
    "\n",
    "from mean_teacher import architectures, datasets, data, losses, ramps, cli\n",
    "from mean_teacher.run_context import RunContext\n",
    "from mean_teacher.data import NO_LABEL\n",
    "from mean_teacher.utils import *\n",
    "\n",
    "import json\n",
    "from data.cifar import CIFAR10, CIFAR100\n",
    "from data.mnist import MNIST\n",
    "\n",
    "\n",
    "import resnet\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# writer = SummaryWriter('runs/supervised_only+self+uniform_noise_40%')\n",
    "\n",
    "LOG = logging.getLogger('main')\n",
    "\n",
    "args = None\n",
    "best_prec1 = 0\n",
    "global_step = 0\n",
    "\n",
    "\n",
    "ensemble_preds = torch.Tensor(np.zeros((50000,10))).cuda()\n",
    "how_many_labels_filtered = torch.Tensor(np.zeros((10))).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(train_transformation,\n",
    "                        eval_transformation,\n",
    "                        datadir,\n",
    "                        args):\n",
    "    args.noise_rate = 0.4\n",
    "    # load dataset\n",
    "    if args.dataset == 'mnist':\n",
    "        input_channel = 1\n",
    "        num_classes = 10\n",
    "        args.top_bn = False\n",
    "        args.epoch_decay_start = 80\n",
    "        args.n_epoch = 200\n",
    "        train_dataset = MNIST(root='./data/',\n",
    "                              download=True,\n",
    "                              train=True,\n",
    "                              transform=train_transformation,\n",
    "                              noise_type=args.noise_type,\n",
    "                              noise_rate=args.noise_rate\n",
    "                              )\n",
    "\n",
    "        test_dataset = MNIST(root='./data/',\n",
    "                             download=True,\n",
    "                             train=False,\n",
    "                             transform=eval_transformation,\n",
    "                             noise_type=args.noise_type,\n",
    "                             noise_rate=args.noise_rate\n",
    "                             )\n",
    "\n",
    "    if args.dataset == 'cifar10':\n",
    "        input_channel = 3\n",
    "        num_classes = 10\n",
    "        args.top_bn = False\n",
    "        args.epoch_decay_start = 80\n",
    "        args.n_epoch = 200\n",
    "        train_dataset = CIFAR10(root='./data/',\n",
    "                                download=None,\n",
    "                                train=True,\n",
    "                                transform=train_transformation,\n",
    "                                noise_type=args.noise_type,\n",
    "                                noise_rate=args.noise_rate\n",
    "                                )\n",
    "\n",
    "        test_dataset = CIFAR10(root='./data/',\n",
    "                               download=None,\n",
    "                               train=False,\n",
    "                               transform=eval_transformation,\n",
    "                               noise_type=args.noise_type,\n",
    "                               noise_rate=args.noise_rate\n",
    "                               )\n",
    "\n",
    "    if args.dataset == 'cifar100':\n",
    "        input_channel = 3\n",
    "        num_classes = 100\n",
    "        args.top_bn = False\n",
    "        args.epoch_decay_start = 100\n",
    "        args.n_epoch = 200\n",
    "        train_dataset = CIFAR100(root='./data/',\n",
    "                                 download=True,\n",
    "                                 train=True,\n",
    "                                 transform=train_transformation,\n",
    "                                 noise_type=args.noise_type,\n",
    "                                 noise_rate=args.noise_rate\n",
    "                                 )\n",
    "\n",
    "        test_dataset = CIFAR100(root='./data/',\n",
    "                                download=True,\n",
    "                                train=False,\n",
    "                                transform=eval_transformation,\n",
    "                                noise_type=args.noise_type,\n",
    "                                noise_rate=args.noise_rate\n",
    "                                )\n",
    "\n",
    "    print('loading dataset...')\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=args.batch_size,\n",
    "                                               num_workers=args.num_workers,\n",
    "                                               drop_last=True,\n",
    "                                               shuffle=True,\n",
    "                                               pin_memory=True)\n",
    "\n",
    "    eval_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                              batch_size=args.batch_size,\n",
    "                                              num_workers=2*args.num_workers,\n",
    "                                              drop_last=False,\n",
    "                                              shuffle=False,\n",
    "                                              pin_memory=True)\n",
    "\n",
    "    return train_loader, eval_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dict_args(**kwargs):\n",
    "    global args\n",
    "\n",
    "    def to_cmdline_kwarg(key, value):\n",
    "        if len(key) == 1:\n",
    "            key = \"-{}\".format(key)\n",
    "        else:\n",
    "            key = \"--{}\".format(re.sub(r\"_\", \"-\", key))\n",
    "        value = str(value)\n",
    "        return key, value\n",
    "\n",
    "    kwargs_pairs = (to_cmdline_kwarg(key, value)\n",
    "                    for key, value in kwargs.items())\n",
    "    cmdline_args = list(sum(kwargs_pairs, ()))\n",
    "    args = parser.parse_args(cmdline_args)\n",
    "    \n",
    "def adjust_learning_rate(optimizer, epoch, step_in_epoch, total_steps_in_epoch):\n",
    "    lr = args.lr\n",
    "    epoch = epoch + step_in_epoch / total_steps_in_epoch\n",
    "\n",
    "    # LR warm-up to handle large minibatch sizes from https://arxiv.org/abs/1706.02677\n",
    "    lr = ramps.linear_rampup(epoch, args.lr_rampup) * (args.lr - args.initial_lr) + args.initial_lr\n",
    "\n",
    "    # Cosine LR rampdown from https://arxiv.org/abs/1608.03983 (but one cycle only)\n",
    "    if args.lr_rampdown_epochs:\n",
    "        assert args.lr_rampdown_epochs >= args.epochs\n",
    "        lr *= ramps.cosine_rampdown(epoch, args.lr_rampdown_epochs)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "def get_current_consistency_weight(epoch):\n",
    "    # Consistency ramp-up from https://arxiv.org/abs/1610.02242\n",
    "    return args.consistency * ramps.sigmoid_rampup(epoch, args.consistency_rampup)\n",
    "\n",
    "def accuracy(output, target, topk=(1,)): # topk? the num of labels which have the highest confidence from logit val(list)\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk) # how many labels will be extracted from each logit?\n",
    "    labeled_minibatch_size = max(target.ne(NO_LABEL).sum(), 1e-8) # num of labeled data\n",
    "    labeled_minibatch_size = labeled_minibatch_size.float() # dtype from int64 to float32 => correction\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True) # logits(128,10) => extract the labels which have the top5(=maxk) highest confidence\n",
    "    pred = pred.t() # shape from (128,5) to (5,128)\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred)) # target.view(1,-1).expand_as(pred) => make target to have the shape of (5,128) like that of pred\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / labeled_minibatch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter_labels(clean_idxs):\n",
    "    global how_many_labels_filtered\n",
    "\n",
    "    clean_idxs = clean_idxs.tolist()\n",
    "    for i in range(10):\n",
    "        how_many_labels_filtered[i] += clean_idxs.count(i)\n",
    "\n",
    "    return how_many_labels_filtered\n",
    "\n",
    "def filtering(indexes, labels):\n",
    "    global ensemble_preds\n",
    "\n",
    "    clean_idxs = np.argwhere(np.argmax(ensemble_preds[indexes].cpu().detach(),axis=1) == labels.cpu())\n",
    "    clean_idxs = clean_idxs[0] # idxs of clean data in minibatch 128\n",
    "    writer.add_scalar('len_clean_idxs', len(clean_idxs), global_step=global_step)\n",
    "    return clean_idxs\n",
    "\n",
    "def update_ensemble_preds(logit1, indexes):\n",
    "    global ensemble_preds\n",
    "    global ema_ensemble_preds\n",
    "\n",
    "    alpha = 0.99\n",
    "    ensemble_preds[indexes] = alpha * ensemble_preds[indexes] + (1-alpha)*logit1\n",
    "\n",
    "def loss_by_filtered_samples(logit1, labels, indexes):\n",
    "    global how_many_labels_filtered\n",
    "\n",
    "    update_ensemble_preds(logit1, indexes)\n",
    "    clean_idx = filtering(indexes, labels)\n",
    "\n",
    "    logit1_update = logit1[clean_idx]\n",
    "    labels_update = labels[clean_idx]\n",
    "    counter_labels(labels_update)\n",
    "\n",
    "    size = len(clean_idx) # 0~128\n",
    "    # class_loss\n",
    "    class_criterion = nn.CrossEntropyLoss(size_average=False).cuda()\n",
    "    class_loss = class_criterion(logit1_update, labels_update.cuda()) / size\n",
    "\n",
    "#     writer.add_histogram('how_many_labels_filtered', how_many_labels_filtered, global_step=global_step)\n",
    "    return class_loss, logit1_update, labels_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "from mean_teacher.cli import architectures, datasets\n",
    "\n",
    "LOG = logging.getLogger('main')\n",
    "\n",
    "__all__ = ['parse_cmd_args', 'parse_dict_args']\n",
    "\n",
    "def create_parser():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch CIFAR-10 Training')\n",
    "\n",
    "    parser.add_argument('--train-subdir', type=str, default='train+val',################################################################################\n",
    "                        help='the subdirectory inside the data directory that contains the training data')\n",
    "    parser.add_argument('--eval-subdir', type=str, default='test',################################################################################\n",
    "                        help='the subdirectory inside the data directory that contains the evaluation data')\n",
    "    parser.add_argument('--labels', default='data-local/labels/cifar10/1000_balanced_labels/00.txt', type=str, metavar='FILE', ################################################################################\n",
    "                        help='list of image labels (default: based on directory structure)')\n",
    "    parser.add_argument('--arch', '-a', metavar='ARCH', default='cifar_shakeshake26', ################################################################################\n",
    "                        choices=architectures.__all__,\n",
    "                        help='model architecture: ' +\n",
    "                            ' | '.join(architectures.__all__))\n",
    "    parser.add_argument('-j', '--num_workers', default=4, type=int, metavar='N',\n",
    "                        help='number of data loading workers (default: 4)')\n",
    "    parser.add_argument('--epochs', default=600, type=int, metavar='N', ################################################################################\n",
    "                        help='number of total epochs to run')\n",
    "    parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                        help='manual epoch number (useful on restarts)')\n",
    "    parser.add_argument('-b', '--batch-size', default=128, type=int, ################################################################################\n",
    "                        metavar='N', help='mini-batch size (default: 256)')\n",
    "    parser.add_argument('--lr', '--learning-rate', default=0.05, type=float,\n",
    "                        metavar='LR', help='max learning rate')\n",
    "    parser.add_argument('--initial-lr', default=0.0, type=float,\n",
    "                        metavar='LR', help='initial learning rate when using linear rampup')\n",
    "    parser.add_argument('--lr-rampup', default=0, type=int, metavar='EPOCHS',\n",
    "                        help='length of learning rate rampup in the beginning')\n",
    "    parser.add_argument('--lr-rampdown-epochs', default=700, type=int, metavar='EPOCHS',\n",
    "                        help='length of learning rate cosine rampdown (>= length of training)')\n",
    "    parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                        help='momentum')\n",
    "    parser.add_argument('--nesterov', default=True, type=str2bool,\n",
    "                        help='use nesterov momentum', metavar='BOOL')\n",
    "    parser.add_argument('--weight-decay', '--wd', default=2e-4, type=float, ####################### 1e-4 => 2e-4 according to the paper\n",
    "                        metavar='W', help='weight decay (default: 1e-4)')\n",
    "    parser.add_argument('--ema-decay', default=0.97, type=float, metavar='ALPHA', ########################## 0.999 => 0.97 according to the paper\n",
    "                        help='ema variable decay rate (default: 0.999)')\n",
    "    parser.add_argument('--consistency', default=100.0, type=float, metavar='WEIGHT',\n",
    "                        help='use consistency loss with given weight (default: None)')\n",
    "    parser.add_argument('--consistency-type', default=\"mse\", type=str, metavar='TYPE',\n",
    "                        choices=['mse', 'kl'],\n",
    "                        help='consistency loss type to use')\n",
    "    parser.add_argument('--consistency-rampup', default=5, type=int, metavar='EPOCHS',\n",
    "                        help='length of the consistency loss ramp-up')\n",
    "    parser.add_argument('--logit-distance-cost', default=-1, type=float, metavar='WEIGHT',\n",
    "                        help='let the student model have two outputs and use an MSE loss between the logits with the given weight (default: only have one output)')\n",
    "    parser.add_argument('--checkpoint-epochs', default=20, type=int,\n",
    "                        metavar='EPOCHS', help='checkpoint frequency in epochs, 0 to turn checkpointing off (default: 1)')\n",
    "    parser.add_argument('--evaluation-epochs', default=1, type=int,\n",
    "                        metavar='EPOCHS', help='evaluation frequency in epochs, 0 to turn evaluation off (default: 1)')\n",
    "    parser.add_argument('--print-freq', '-p', default=10, type=int,\n",
    "                        metavar='N', help='print frequency (default: 10)')\n",
    "    parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                        help='path to latest checkpoint (default: none)')\n",
    "    parser.add_argument('-e', '--evaluate', type=str2bool,\n",
    "                        help='evaluate model on evaluation set')\n",
    "    parser.add_argument('--pretrained', dest='pretrained', action='store_true',\n",
    "                        help='use pre-trained model')\n",
    "    parser.add_argument('--dataset', type=str, help='mnist, cifar10, or cifar100', default='cifar10')\n",
    "    parser.add_argument('--noise_rate', type=float, help='corruption rate, should be less than 1', default=0.0)\n",
    "    parser.add_argument('--forget_rate', type=float, help='forget rate', default=None)\n",
    "    parser.add_argument('--noise_type', type=str, help='[pairflip, symmetric]', default='symmetric')\n",
    "    parser.add_argument('--num_gradual', type=int, default=10,\n",
    "                        help='how many epochs for linear drop rate, can be 5, 10, 15. This parameter is equal to Tk for R(T) in Co-teaching paper.')\n",
    "    parser.add_argument('--exponent', type=float, default=1,\n",
    "                        help='exponent of the forget rate, can be 0.5, 1, 2. This parameter is equal to c in Tc for R(T) in Co-teaching paper.')\n",
    "    parser.add_argument('--top_bn', action='store_true')\n",
    "    parser.add_argument('--seed', type=int, default=1)\n",
    "    parser.add_argument('--print_freq', type=int, default=50)\n",
    "    parser.add_argument('--num_iter_per_epoch', type=int, default=400)\n",
    "    parser.add_argument('--epoch_decay_start', type=int, default=80)\n",
    "\n",
    "    return parser\n",
    "\n",
    "def parse_commandline_args():\n",
    "    return create_parser().parse_args()\n",
    "\n",
    "\n",
    "def parse_dict_args(**kwargs):\n",
    "    def to_cmdline_kwarg(key, value):\n",
    "        if len(key) == 1:\n",
    "            key = \"-{}\".format(key)\n",
    "        else:\n",
    "            key = \"--{}\".format(re.sub(r\"_\", \"-\", key))\n",
    "        value = str(value)\n",
    "        return key, value\n",
    "\n",
    "    kwargs_pairs = (to_cmdline_kwarg(key, value)\n",
    "                    for key, value in kwargs.items())\n",
    "    cmdline_args = list(sum(kwargs_pairs, ()))\n",
    "\n",
    "    LOG.info(\"Using these command line args: %s\", \" \".join(cmdline_args))\n",
    "\n",
    "    return create_parser().parse_args(cmdline_args)\n",
    "\n",
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "\n",
    "def str2epochs(v):\n",
    "    try:\n",
    "        if len(v) == 0:\n",
    "            epochs = []\n",
    "        else:\n",
    "            epochs = [int(string) for string in v.split(\",\")]\n",
    "    except:\n",
    "        raise argparse.ArgumentTypeError(\n",
    "            'Expected comma-separated list of integers, got \"{}\"'.format(v))\n",
    "    if not all(0 < epoch1 < epoch2 for epoch1, epoch2 in zip(epochs[:-1], epochs[1:])):\n",
    "        raise argparse.ArgumentTypeError(\n",
    "            'Expected the epochs to be listed in increasing order')\n",
    "    return epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    args, argv = create_parser().parse_known_args(None, None)\n",
    "#     args = cli.parse_commandline_args()\n",
    "#     main(RunContext(__file__, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'indexes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4d1fd2de9c67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindexes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'indexes' is not defined"
     ]
    }
   ],
   "source": [
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 6, 8, 9, 10]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a=[0,0,0,1,1,0,1,0,1,1,1,0]\n",
    "b=[0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "c=list()\n",
    "for i in range(len(b)):\n",
    "    if a[i] == 1:\n",
    "        c.append(b[i])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, epoch, log):\n",
    "    global global_step\n",
    "\n",
    "    meters = AverageMeterSet()\n",
    "    model.train()\n",
    "\n",
    "    if epoch == 0:\n",
    "        for i, ((input, _), labels, indexes) in enumerate(train_loader):\n",
    "\n",
    "            class_criterion = nn.CrossEntropyLoss(size_average=False).cuda()\n",
    "\n",
    "            adjust_learning_rate(optimizer, epoch, i, len(train_loader))\n",
    "            meters.update('lr', optimizer.param_groups[0]['lr'])\n",
    "\n",
    "            input_var = torch.autograd.Variable(input)\n",
    "            target_var = torch.autograd.Variable(labels.cuda(non_blocking=True))\n",
    "\n",
    "            minibatch_size = len(target_var)  # 128\n",
    "            model_out = model(input_var)  # tuple(len:2), [128,10], [128,10]\n",
    "\n",
    "            class_loss = class_criterion(model_out, target_var) / minibatch_size\n",
    "            meters.update('class_loss', class_loss.item())\n",
    "\n",
    "            loss = class_loss\n",
    "            assert not (np.isnan(loss.item()) or loss.item() > 1e5), 'Loss explosion: {}'.format(loss.item())\n",
    "            meters.update('loss', loss.item())\n",
    "\n",
    "            prec1, prec5 = accuracy(model_out.data, target_var.data, topk=(1, 5))\n",
    "            meters.update('top1', prec1[0], minibatch_size)\n",
    "            meters.update('error1', 100. - prec1[0], minibatch_size)\n",
    "            meters.update('top5', prec5[0], minibatch_size)\n",
    "            meters.update('error5', 100. - prec5[0], minibatch_size)\n",
    "\n",
    "            # compute gradient and do SGD step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "\n",
    "            if i % args.print_freq == 0:\n",
    "                LOG.info(\n",
    "                        'Epoch: [{0}][{1}/{2}]\\t'\n",
    "                        'Class {meters[class_loss]:.4f}\\t'\n",
    "                        'Prec@1 {meters[top1]:.3f}\\t'\n",
    "                        'Prec@5 {meters[top5]:.3f}'.format(\n",
    "                            epoch, i, len(train_loader), meters=meters))\n",
    "                log.record(epoch + i / len(train_loader), {\n",
    "                        'step': global_step,\n",
    "                        **meters.values(),\n",
    "                        **meters.averages(),\n",
    "                        **meters.sums()\n",
    "                })\n",
    "    else:\n",
    "        for i, ((input, _), labels, indexes) in enumerate(train_loader):\n",
    "\n",
    "            adjust_learning_rate(optimizer, epoch, i, len(train_loader))\n",
    "            meters.update('lr', optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "#             filtered = list()\n",
    "#             for idx in indexes:\n",
    "#                 if filtered_labels[idx] == 1:\n",
    "#                     filtered.append(indexes[idx])\n",
    "#             indexes = filtered\n",
    "            input_var = torch.autograd.Variable(input)\n",
    "            target_var = torch.autograd.Variable(labels.cuda(non_blocking=True))\n",
    "\n",
    "            a = list()\n",
    "            for idx in indexes:\n",
    "                a.append(filtered_labels[idx])\n",
    "            num = 0\n",
    "            for i in range(len(a)):\n",
    "                i -= num\n",
    "                if a[i] == 0:\n",
    "                    del input_var[i]\n",
    "                    del target_var[i]\n",
    "                    num += 1\n",
    "\n",
    "            model_out = model(input_var)  # tuple(len:2), [128,10], [128,10]\n",
    "\n",
    "            class_criterion = nn.CrossEntropyLoss(size_average=False).cuda()\n",
    "            class_loss = class_criterion(model_out, target_var.cuda()) / size\n",
    "\n",
    "            minibatch_size = len(labels_update)\n",
    "\n",
    "            # class_loss = class_criterion(class_logit, target_var) / minibatch_size\n",
    "            meters.update('class_loss', class_loss.item())\n",
    "            meters.update('loss', loss.item())\n",
    "\n",
    "            prec1, prec5 = accuracy(model_out_update.data, labels_update.data, topk=(1, 5))\n",
    "            meters.update('top1', prec1[0], minibatch_size)\n",
    "            meters.update('error1', 100. - prec1[0], minibatch_size)\n",
    "            meters.update('top5', prec5[0], minibatch_size)\n",
    "            meters.update('error5', 100. - prec5[0], minibatch_size)\n",
    "\n",
    "            # compute gradient and do SGD step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "\n",
    "            if i % args.print_freq == 0:\n",
    "                LOG.info(\n",
    "                    'Epoch: [{0}][{1}/{2}]\\t'\n",
    "                    'Class {meters[class_loss]:.4f}\\t'\n",
    "                    'Prec@1 {meters[top1]:.3f}\\t'\n",
    "                    'Prec@5 {meters[top5]:.3f}'.format(\n",
    "                        epoch, i, len(train_loader), meters=meters))\n",
    "                log.record(epoch + i / len(train_loader), {\n",
    "                    'step': global_step,\n",
    "                    **meters.values(),\n",
    "                    **meters.averages(),\n",
    "                    **meters.sums()\n",
    "                })\n",
    "    filtered_labels = torch.Tensor(np.zeros((50000,10))).cuda()\n",
    "    for i, ((input, _), labels, indexes) in enumerate(train_loader):\n",
    "            \n",
    "            input_var = torch.autograd.Variable(input)\n",
    "            target_var = torch.autograd.Variable(labels.cuda(non_blocking=True))\n",
    "            model_out = model(input_var)\n",
    "            labels_update = loss_by_filtered_samples(model_out, target_var, indexes)         \n",
    "        \n",
    "#     writer.add_scalar('loss', meters['loss'].avg, global_step=global_step)\n",
    "\n",
    "def validate(eval_loader, model, log, global_step, epoch):\n",
    "    with torch.no_grad():\n",
    "        class_criterion = nn.CrossEntropyLoss(size_average=False).cuda()\n",
    "        meters = AverageMeterSet()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for i, (input, target, _) in enumerate(eval_loader): # len(eval_loader) = 79 ( 128*(79-1) + 16*1 = 10000(test_imgs) ). num of each labels is 1000. all labeled\n",
    "\n",
    "            input_var = torch.autograd.Variable(input, volatile=True) # [128,3,32,32]\n",
    "            target_var = torch.autograd.Variable(target.cuda(non_blocking=True), volatile=True) # [128]\n",
    "\n",
    "            minibatch_size = len(target_var)\n",
    "\n",
    "            output1 = model(input_var)\n",
    "            class_loss = class_criterion(output1, target_var) / minibatch_size # why does cross_entropy between output and target_var not between softmax and target_var? => In class_criterion, it will be done 'F.log_softmax' in the end\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1, prec5 = accuracy(output1.data, target_var.data, topk=(1, 5))\n",
    "            meters.update('class_loss', class_loss.item(), minibatch_size)\n",
    "            meters.update('top1', prec1[0], minibatch_size)\n",
    "            meters.update('error1', 100.0 - prec1[0], minibatch_size)\n",
    "            meters.update('top5', prec5[0], minibatch_size)\n",
    "            meters.update('error5', 100.0 - prec5[0], minibatch_size)\n",
    "\n",
    "            if i % args.print_freq == 0:\n",
    "                LOG.info(\n",
    "                    'Test: [{0}/{1}]\\t'\n",
    "                    'Class {meters[class_loss]:.4f}\\t'\n",
    "                    'Prec@1 {meters[top1]:.3f}\\t'\n",
    "                    'Prec@5 {meters[top5]:.3f}'.format(\n",
    "                        i, len(eval_loader), meters=meters))\n",
    "\n",
    "        LOG.info(' * Prec@1 {top1.avg:.3f}\\tPrec@5 {top5.avg:.3f}'\n",
    "              .format(top1=meters['top1'], top5=meters['top5']))\n",
    "        log.record(epoch, {\n",
    "            'step': global_step,\n",
    "            **meters.values(),\n",
    "            **meters.averages(),\n",
    "            **meters.sums()\n",
    "        })\n",
    "#         writer.add_scalar('test_acc', meters['top1'].avg, global_step=global_step)\n",
    "    return meters['top1'].avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import pyarrow\n",
    "\n",
    "from pandas import DataFrame\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class TrainLog:\n",
    "    \"\"\"Saves training logs in Pandas msgpacks\"\"\"\n",
    "\n",
    "    INCREMENTAL_UPDATE_TIME = 300\n",
    "\n",
    "    def __init__(self, directory, name):\n",
    "        self.log_file_path = \"{}/{}.msgpack\".format(directory, name)\n",
    "        self._log = defaultdict(dict)\n",
    "        self._log_lock = threading.RLock()\n",
    "        self._last_update_time = time.time() - self.INCREMENTAL_UPDATE_TIME\n",
    "\n",
    "    def record_single(self, step, column, value):\n",
    "        self._record(step, {column: value})\n",
    "\n",
    "    def record(self, step, col_val_dict):\n",
    "        self._record(step, col_val_dict)\n",
    "\n",
    "    def save(self):\n",
    "        df = self._as_dataframe()\n",
    "        # df.to_msgpack(self.log_file_path, compress='zlib')\n",
    "        context = pyarrow.default_serialization_context()\n",
    "        df = context.serialize(df).to_buffer().to_pybytes()\n",
    "\n",
    "    def _record(self, step, col_val_dict):\n",
    "        with self._log_lock:\n",
    "            self._log[step].update(col_val_dict)\n",
    "            if time.time() - self._last_update_time >= self.INCREMENTAL_UPDATE_TIME:\n",
    "                self._last_update_time = time.time()\n",
    "                self.save()\n",
    "\n",
    "    def _as_dataframe(self):\n",
    "        with self._log_lock:\n",
    "            return DataFrame.from_dict(self._log, orient='index')\n",
    "\n",
    "class RunContext:\n",
    "    \"\"\"Creates directories and files for the run\"\"\"\n",
    "\n",
    "    def __init__(self, runner_file, run_idx):\n",
    "        logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "        runner_name = os.path.basename(runner_file).split(\".\")[0] # runner_name = 'main_supervised+self'\n",
    "        self.result_dir = \"{root}/{runner_name}/{date:%Y-%m-%d_%H:%M:%S}/{run_idx}\".format(\n",
    "            root='results',\n",
    "            runner_name=runner_name,\n",
    "            date=datetime.now(),\n",
    "            run_idx=run_idx\n",
    "        ) # 'results/main_supervised+self/date:2020-07-16_14:00:57/0'\n",
    "        self.transient_dir = self.result_dir + \"/transient\"\n",
    "        os.makedirs(self.result_dir)\n",
    "        os.makedirs(self.transient_dir)\n",
    "\n",
    "    def create_train_log(self, name):\n",
    "        return TrainLog(self.result_dir, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 10\n",
      "50000\n",
      "Actual noise 0.40\n",
      "[[0.6        0.04444444 0.04444444 0.04444444 0.04444444 0.04444444\n",
      "  0.04444444 0.04444444 0.04444444 0.04444444]\n",
      " [0.04444444 0.6        0.04444444 0.04444444 0.04444444 0.04444444\n",
      "  0.04444444 0.04444444 0.04444444 0.04444444]\n",
      " [0.04444444 0.04444444 0.6        0.04444444 0.04444444 0.04444444\n",
      "  0.04444444 0.04444444 0.04444444 0.04444444]\n",
      " [0.04444444 0.04444444 0.04444444 0.6        0.04444444 0.04444444\n",
      "  0.04444444 0.04444444 0.04444444 0.04444444]\n",
      " [0.04444444 0.04444444 0.04444444 0.04444444 0.6        0.04444444\n",
      "  0.04444444 0.04444444 0.04444444 0.04444444]\n",
      " [0.04444444 0.04444444 0.04444444 0.04444444 0.04444444 0.6\n",
      "  0.04444444 0.04444444 0.04444444 0.04444444]\n",
      " [0.04444444 0.04444444 0.04444444 0.04444444 0.04444444 0.04444444\n",
      "  0.6        0.04444444 0.04444444 0.04444444]\n",
      " [0.04444444 0.04444444 0.04444444 0.04444444 0.04444444 0.04444444\n",
      "  0.04444444 0.6        0.04444444 0.04444444]\n",
      " [0.04444444 0.04444444 0.04444444 0.04444444 0.04444444 0.04444444\n",
      "  0.04444444 0.04444444 0.6        0.04444444]\n",
      " [0.04444444 0.04444444 0.04444444 0.04444444 0.04444444 0.04444444\n",
      "  0.04444444 0.04444444 0.04444444 0.6       ]]\n",
      "loading dataset...\n"
     ]
    }
   ],
   "source": [
    "global global_step\n",
    "global best_prec1\n",
    "global ensemble_preds\n",
    "\n",
    "context = RunContext('main_supervised+self', 0)\n",
    "\n",
    "training_log = context.create_train_log(\"training\")\n",
    "validation_log = context.create_train_log(\"validation\")\n",
    "\n",
    "dataset_config = datasets.__dict__[args.dataset]()\n",
    "num_classes = dataset_config.pop('num_classes')\n",
    "train_loader, eval_loader = create_data_loaders(**dataset_config, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:main:\n",
      "List of model parameters:\n",
      "=========================\n",
      "module.conv1.weight                            64 * 3 * 3 * 3 =       1,728\n",
      "module.bn1.weight                                          64 =          64\n",
      "module.bn1.bias                                            64 =          64\n",
      "module.layer1.0.conv1.weight                  64 * 64 * 3 * 3 =      36,864\n",
      "module.layer1.0.bn1.weight                                 64 =          64\n",
      "module.layer1.0.bn1.bias                                   64 =          64\n",
      "module.layer1.0.conv2.weight                  64 * 64 * 3 * 3 =      36,864\n",
      "module.layer1.0.bn2.weight                                 64 =          64\n",
      "module.layer1.0.bn2.bias                                   64 =          64\n",
      "module.layer1.1.conv1.weight                  64 * 64 * 3 * 3 =      36,864\n",
      "module.layer1.1.bn1.weight                                 64 =          64\n",
      "module.layer1.1.bn1.bias                                   64 =          64\n",
      "module.layer1.1.conv2.weight                  64 * 64 * 3 * 3 =      36,864\n",
      "module.layer1.1.bn2.weight                                 64 =          64\n",
      "module.layer1.1.bn2.bias                                   64 =          64\n",
      "module.layer1.2.conv1.weight                  64 * 64 * 3 * 3 =      36,864\n",
      "module.layer1.2.bn1.weight                                 64 =          64\n",
      "module.layer1.2.bn1.bias                                   64 =          64\n",
      "module.layer1.2.conv2.weight                  64 * 64 * 3 * 3 =      36,864\n",
      "module.layer1.2.bn2.weight                                 64 =          64\n",
      "module.layer1.2.bn2.bias                                   64 =          64\n",
      "module.layer2.0.conv1.weight                 128 * 64 * 3 * 3 =      73,728\n",
      "module.layer2.0.bn1.weight                                128 =         128\n",
      "module.layer2.0.bn1.bias                                  128 =         128\n",
      "module.layer2.0.conv2.weight                128 * 128 * 3 * 3 =     147,456\n",
      "module.layer2.0.bn2.weight                                128 =         128\n",
      "module.layer2.0.bn2.bias                                  128 =         128\n",
      "module.layer2.0.shortcut.0.weight            128 * 64 * 1 * 1 =       8,192\n",
      "module.layer2.0.shortcut.1.weight                         128 =         128\n",
      "module.layer2.0.shortcut.1.bias                           128 =         128\n",
      "module.layer2.1.conv1.weight                128 * 128 * 3 * 3 =     147,456\n",
      "module.layer2.1.bn1.weight                                128 =         128\n",
      "module.layer2.1.bn1.bias                                  128 =         128\n",
      "module.layer2.1.conv2.weight                128 * 128 * 3 * 3 =     147,456\n",
      "module.layer2.1.bn2.weight                                128 =         128\n",
      "module.layer2.1.bn2.bias                                  128 =         128\n",
      "module.layer2.2.conv1.weight                128 * 128 * 3 * 3 =     147,456\n",
      "module.layer2.2.bn1.weight                                128 =         128\n",
      "module.layer2.2.bn1.bias                                  128 =         128\n",
      "module.layer2.2.conv2.weight                128 * 128 * 3 * 3 =     147,456\n",
      "module.layer2.2.bn2.weight                                128 =         128\n",
      "module.layer2.2.bn2.bias                                  128 =         128\n",
      "module.layer2.3.conv1.weight                128 * 128 * 3 * 3 =     147,456\n",
      "module.layer2.3.bn1.weight                                128 =         128\n",
      "module.layer2.3.bn1.bias                                  128 =         128\n",
      "module.layer2.3.conv2.weight                128 * 128 * 3 * 3 =     147,456\n",
      "module.layer2.3.bn2.weight                                128 =         128\n",
      "module.layer2.3.bn2.bias                                  128 =         128\n",
      "module.layer3.0.conv1.weight                256 * 128 * 3 * 3 =     294,912\n",
      "module.layer3.0.bn1.weight                                256 =         256\n",
      "module.layer3.0.bn1.bias                                  256 =         256\n",
      "module.layer3.0.conv2.weight                256 * 256 * 3 * 3 =     589,824\n",
      "module.layer3.0.bn2.weight                                256 =         256\n",
      "module.layer3.0.bn2.bias                                  256 =         256\n",
      "module.layer3.0.shortcut.0.weight           256 * 128 * 1 * 1 =      32,768\n",
      "module.layer3.0.shortcut.1.weight                         256 =         256\n",
      "module.layer3.0.shortcut.1.bias                           256 =         256\n",
      "module.layer3.1.conv1.weight                256 * 256 * 3 * 3 =     589,824\n",
      "module.layer3.1.bn1.weight                                256 =         256\n",
      "module.layer3.1.bn1.bias                                  256 =         256\n",
      "module.layer3.1.conv2.weight                256 * 256 * 3 * 3 =     589,824\n",
      "module.layer3.1.bn2.weight                                256 =         256\n",
      "module.layer3.1.bn2.bias                                  256 =         256\n",
      "module.layer3.2.conv1.weight                256 * 256 * 3 * 3 =     589,824\n",
      "module.layer3.2.bn1.weight                                256 =         256\n",
      "module.layer3.2.bn1.bias                                  256 =         256\n",
      "module.layer3.2.conv2.weight                256 * 256 * 3 * 3 =     589,824\n",
      "module.layer3.2.bn2.weight                                256 =         256\n",
      "module.layer3.2.bn2.bias                                  256 =         256\n",
      "module.layer3.3.conv1.weight                256 * 256 * 3 * 3 =     589,824\n",
      "module.layer3.3.bn1.weight                                256 =         256\n",
      "module.layer3.3.bn1.bias                                  256 =         256\n",
      "module.layer3.3.conv2.weight                256 * 256 * 3 * 3 =     589,824\n",
      "module.layer3.3.bn2.weight                                256 =         256\n",
      "module.layer3.3.bn2.bias                                  256 =         256\n",
      "module.layer3.4.conv1.weight                256 * 256 * 3 * 3 =     589,824\n",
      "module.layer3.4.bn1.weight                                256 =         256\n",
      "module.layer3.4.bn1.bias                                  256 =         256\n",
      "module.layer3.4.conv2.weight                256 * 256 * 3 * 3 =     589,824\n",
      "module.layer3.4.bn2.weight                                256 =         256\n",
      "module.layer3.4.bn2.bias                                  256 =         256\n",
      "module.layer3.5.conv1.weight                256 * 256 * 3 * 3 =     589,824\n",
      "module.layer3.5.bn1.weight                                256 =         256\n",
      "module.layer3.5.bn1.bias                                  256 =         256\n",
      "module.layer3.5.conv2.weight                256 * 256 * 3 * 3 =     589,824\n",
      "module.layer3.5.bn2.weight                                256 =         256\n",
      "module.layer3.5.bn2.bias                                  256 =         256\n",
      "module.layer4.0.conv1.weight                512 * 256 * 3 * 3 =   1,179,648\n",
      "module.layer4.0.bn1.weight                                512 =         512\n",
      "module.layer4.0.bn1.bias                                  512 =         512\n",
      "module.layer4.0.conv2.weight                512 * 512 * 3 * 3 =   2,359,296\n",
      "module.layer4.0.bn2.weight                                512 =         512\n",
      "module.layer4.0.bn2.bias                                  512 =         512\n",
      "module.layer4.0.shortcut.0.weight           512 * 256 * 1 * 1 =     131,072\n",
      "module.layer4.0.shortcut.1.weight                         512 =         512\n",
      "module.layer4.0.shortcut.1.bias                           512 =         512\n",
      "module.layer4.1.conv1.weight                512 * 512 * 3 * 3 =   2,359,296\n",
      "module.layer4.1.bn1.weight                                512 =         512\n",
      "module.layer4.1.bn1.bias                                  512 =         512\n",
      "module.layer4.1.conv2.weight                512 * 512 * 3 * 3 =   2,359,296\n",
      "module.layer4.1.bn2.weight                                512 =         512\n",
      "module.layer4.1.bn2.bias                                  512 =         512\n",
      "module.layer4.2.conv1.weight                512 * 512 * 3 * 3 =   2,359,296\n",
      "module.layer4.2.bn1.weight                                512 =         512\n",
      "module.layer4.2.bn1.bias                                  512 =         512\n",
      "module.layer4.2.conv2.weight                512 * 512 * 3 * 3 =   2,359,296\n",
      "module.layer4.2.bn2.weight                                512 =         512\n",
      "module.layer4.2.bn2.bias                                  512 =         512\n",
      "module.linear.weight                                 10 * 512 =       5,120\n",
      "module.linear.bias                                         10 =          10\n",
      "===========================================================================\n",
      "all parameters                                   sum of above =  21,282,122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = resnet.ResNet34()\n",
    "model = nn.DataParallel(model).cuda()\n",
    "\n",
    "LOG.info(parameters_string(model))\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "                                momentum=args.momentum,\n",
    "                                weight_decay=args.weight_decay,\n",
    "                                nesterov=args.nesterov)\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:main:Epoch: [0][0/390]\tClass 2.4569 (2.4569)\tPrec@1 9.375 (9.375)\tPrec@5 52.344 (52.344)\n",
      "INFO:main:Epoch: [0][10/390]\tClass 3.5175 (4.6813)\tPrec@1 12.500 (9.659)\tPrec@5 49.219 (49.148)\n",
      "INFO:main:Epoch: [0][20/390]\tClass 2.3872 (3.6623)\tPrec@1 8.594 (10.417)\tPrec@5 55.469 (49.888)\n",
      "INFO:main:Epoch: [0][30/390]\tClass 2.3182 (3.2466)\tPrec@1 10.938 (10.207)\tPrec@5 55.469 (50.630)\n",
      "INFO:main:Epoch: [0][40/390]\tClass 2.3306 (3.0268)\tPrec@1 10.938 (10.575)\tPrec@5 50.000 (50.934)\n",
      "INFO:main:Epoch: [0][50/390]\tClass 2.3411 (2.8878)\tPrec@1 13.281 (10.738)\tPrec@5 53.906 (51.670)\n",
      "INFO:main:Epoch: [0][60/390]\tClass 2.3157 (2.7921)\tPrec@1 12.500 (11.053)\tPrec@5 56.250 (52.049)\n",
      "INFO:main:Epoch: [0][70/390]\tClass 2.3817 (2.7249)\tPrec@1 11.719 (11.466)\tPrec@5 49.219 (52.740)\n",
      "INFO:main:Epoch: [0][80/390]\tClass 2.2927 (2.6696)\tPrec@1 15.625 (11.834)\tPrec@5 55.469 (53.877)\n",
      "INFO:main:Epoch: [0][90/390]\tClass 2.2649 (2.6266)\tPrec@1 21.875 (12.354)\tPrec@5 62.500 (54.636)\n",
      "INFO:main:Epoch: [0][100/390]\tClass 2.2327 (2.5910)\tPrec@1 19.531 (12.740)\tPrec@5 65.625 (55.538)\n",
      "INFO:main:Epoch: [0][110/390]\tClass 2.2228 (2.5623)\tPrec@1 14.844 (12.972)\tPrec@5 60.938 (56.123)\n",
      "INFO:main:Epoch: [0][120/390]\tClass 2.2887 (2.5391)\tPrec@1 17.188 (13.120)\tPrec@5 59.375 (56.579)\n",
      "INFO:main:Epoch: [0][130/390]\tClass 2.2739 (2.5171)\tPrec@1 21.094 (13.442)\tPrec@5 57.812 (57.043)\n",
      "INFO:main:Epoch: [0][140/390]\tClass 2.2668 (2.4991)\tPrec@1 23.438 (13.647)\tPrec@5 66.406 (57.563)\n",
      "INFO:main:Epoch: [0][150/390]\tClass 2.3291 (2.4823)\tPrec@1 11.719 (13.773)\tPrec@5 55.469 (58.030)\n",
      "INFO:main:Epoch: [0][160/390]\tClass 2.2649 (2.4667)\tPrec@1 14.844 (13.927)\tPrec@5 67.969 (58.463)\n",
      "INFO:main:Epoch: [0][170/390]\tClass 2.1773 (2.4514)\tPrec@1 21.094 (14.172)\tPrec@5 62.500 (58.932)\n",
      "INFO:main:Epoch: [0][180/390]\tClass 2.2753 (2.4400)\tPrec@1 12.500 (14.365)\tPrec@5 56.250 (59.069)\n",
      "INFO:main:Epoch: [0][190/390]\tClass 2.2872 (2.4295)\tPrec@1 14.844 (14.504)\tPrec@5 60.156 (59.322)\n",
      "INFO:main:Epoch: [0][200/390]\tClass 2.2591 (2.4190)\tPrec@1 14.844 (14.665)\tPrec@5 67.969 (59.717)\n",
      "INFO:main:Epoch: [0][210/390]\tClass 2.1465 (2.4097)\tPrec@1 15.625 (14.777)\tPrec@5 67.969 (59.930)\n",
      "INFO:main:Epoch: [0][220/390]\tClass 2.2571 (2.4022)\tPrec@1 15.625 (14.890)\tPrec@5 65.625 (60.139)\n",
      "INFO:main:Epoch: [0][230/390]\tClass 2.2061 (2.3944)\tPrec@1 21.094 (15.070)\tPrec@5 66.406 (60.332)\n",
      "INFO:main:Epoch: [0][240/390]\tClass 2.2794 (2.3879)\tPrec@1 15.625 (15.165)\tPrec@5 63.281 (60.555)\n",
      "INFO:main:Epoch: [0][250/390]\tClass 2.1724 (2.3820)\tPrec@1 19.531 (15.248)\tPrec@5 67.188 (60.648)\n",
      "INFO:main:Epoch: [0][260/390]\tClass 2.1901 (2.3762)\tPrec@1 21.875 (15.314)\tPrec@5 69.531 (60.860)\n",
      "INFO:main:Epoch: [0][270/390]\tClass 2.1822 (2.3703)\tPrec@1 25.000 (15.455)\tPrec@5 60.156 (61.010)\n",
      "INFO:main:Epoch: [0][280/390]\tClass 2.1501 (2.3652)\tPrec@1 24.219 (15.583)\tPrec@5 69.531 (61.146)\n",
      "INFO:main:Epoch: [0][290/390]\tClass 2.2149 (2.3591)\tPrec@1 21.875 (15.781)\tPrec@5 61.719 (61.338)\n",
      "INFO:main:Epoch: [0][300/390]\tClass 2.2637 (2.3537)\tPrec@1 13.281 (15.931)\tPrec@5 61.719 (61.542)\n",
      "INFO:main:Epoch: [0][310/390]\tClass 2.2054 (2.3490)\tPrec@1 24.219 (16.024)\tPrec@5 69.531 (61.721)\n",
      "INFO:main:Epoch: [0][320/390]\tClass 2.2361 (2.3447)\tPrec@1 17.188 (16.107)\tPrec@5 67.188 (61.901)\n",
      "INFO:main:Epoch: [0][330/390]\tClass 2.2439 (2.3408)\tPrec@1 14.844 (16.187)\tPrec@5 62.500 (62.009)\n",
      "INFO:main:Epoch: [0][340/390]\tClass 2.2226 (2.3368)\tPrec@1 17.969 (16.283)\tPrec@5 66.406 (62.172)\n",
      "INFO:main:Epoch: [0][350/390]\tClass 2.1928 (2.3320)\tPrec@1 20.312 (16.449)\tPrec@5 70.312 (62.371)\n",
      "INFO:main:Epoch: [0][360/390]\tClass 2.2119 (2.3276)\tPrec@1 17.188 (16.608)\tPrec@5 67.188 (62.539)\n",
      "INFO:main:Epoch: [0][370/390]\tClass 2.0809 (2.3246)\tPrec@1 28.906 (16.691)\tPrec@5 78.906 (62.654)\n",
      "INFO:main:Epoch: [0][380/390]\tClass 2.1094 (2.3207)\tPrec@1 25.781 (16.835)\tPrec@5 72.656 (62.834)\n"
     ]
    }
   ],
   "source": [
    "train(train_loader, model, optimizer, epoch, training_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:main:Evaluating the model:\n",
      "/home/taehwan/anaconda3/envs/taehwan/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/taehwan/anaconda3/envs/taehwan/lib/python3.7/site-packages/ipykernel_launcher.py:111: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "INFO:main:Test: [0/79]\tClass 2.0580 (2.0580)\tPrec@1 27.344 (27.344)\tPrec@5 78.906 (78.906)\n",
      "INFO:main:Test: [10/79]\tClass 2.0436 (2.0536)\tPrec@1 25.781 (26.776)\tPrec@5 80.469 (79.332)\n",
      "INFO:main:Test: [20/79]\tClass 2.0688 (2.0510)\tPrec@1 23.438 (26.376)\tPrec@5 78.125 (79.167)\n",
      "INFO:main:Test: [30/79]\tClass 2.0170 (2.0453)\tPrec@1 27.344 (27.092)\tPrec@5 78.125 (79.183)\n",
      "INFO:main:Test: [40/79]\tClass 2.0287 (2.0409)\tPrec@1 28.125 (27.134)\tPrec@5 75.781 (79.402)\n",
      "INFO:main:Test: [50/79]\tClass 2.0927 (2.0432)\tPrec@1 19.531 (27.206)\tPrec@5 72.656 (78.952)\n",
      "INFO:main:Test: [60/79]\tClass 1.9894 (2.0406)\tPrec@1 31.250 (27.228)\tPrec@5 85.938 (79.278)\n",
      "INFO:main:Test: [70/79]\tClass 2.0722 (2.0379)\tPrec@1 24.219 (27.212)\tPrec@5 75.000 (79.335)\n",
      "INFO:main: * Prec@1 27.200\tPrec@5 79.540\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "if args.evaluation_epochs and (epoch + 1) % args.evaluation_epochs == 0:\n",
    "    LOG.info(\"Evaluating the model:\")\n",
    "    prec1 = validate(eval_loader, model, validation_log, global_step, epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (shortcut): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args.evaluate:\n",
    "    LOG.info(\"Evaluating the primary model:\")\n",
    "    validate(eval_loader, model, validation_log, global_step, args.start_epoch)\n",
    "    return\n",
    "\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    train(train_loader, model, optimizer,epoch, training_log)\n",
    "\n",
    "    if args.evaluation_epochs and (epoch + 1) % args.evaluation_epochs == 0:\n",
    "        LOG.info(\"Evaluating the model:\")\n",
    "        prec1 = validate(eval_loader, model, validation_log, global_step, epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_preds = torch.Tensor(np.zeros((50000,10))).cuda()\n",
    "how_many_labels_filtered = torch.Tensor(np.zeros((10))).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter_labels(clean_idxs):\n",
    "    global how_many_labels_filtered\n",
    "\n",
    "    clean_idxs = clean_idxs.tolist()\n",
    "    for i in range(10):\n",
    "        how_many_labels_filtered[i] += clean_idxs.count(i)\n",
    "\n",
    "#     return how_many_labels_filtered\n",
    "\n",
    "def filtering(indexes, labels):\n",
    "    global ensemble_preds\n",
    "    global filtered_labels\n",
    "\n",
    "    clean_idxs = np.argwhere(np.argmax(ensemble_preds[indexes].cpu().detach(),axis=1) == labels.cpu())\n",
    "    clean_idxs = clean_idxs[0] # idxs of clean data in minibatch 128\n",
    "    return clean_idxs\n",
    "\n",
    "def update_ensemble_preds(logit1, indexes):\n",
    "    global ensemble_preds\n",
    "    global ema_ensemble_preds\n",
    "\n",
    "    alpha = 0.99\n",
    "    ensemble_preds[indexes] = alpha * ensemble_preds[indexes] + (1-alpha)*logit1\n",
    "\n",
    "def loss_by_filtered_samples(logit1, labels, indexes):\n",
    "    global how_many_labels_filtered\n",
    "\n",
    "    update_ensemble_preds(logit1, indexes)\n",
    "    clean_idx = filtering(indexes, labels)\n",
    "\n",
    "    logit1_update = logit1[clean_idx]\n",
    "    labels_update = labels[clean_idx]\n",
    "    counter_labels(labels_update)\n",
    "\n",
    "#     size = len(clean_idx) # 0~128\n",
    "    # class_loss\n",
    "#     class_criterion = nn.CrossEntropyLoss(size_average=False).cuda()\n",
    "#     class_loss = class_criterion(logit1_update, labels_update.cuda()) / size\n",
    "\n",
    "#     writer.add_histogram('how_many_labels_filtered', how_many_labels_filtered, global_step=global_step)\n",
    "    return class_loss, logit1_update, labels_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:main:Epoch: [1][0/390]\tClass 1.4875 (1.4875)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][10/390]\tClass 0.0943 (0.4985)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][20/390]\tClass 0.1103 (0.2829)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][30/390]\tClass 0.0236 (0.1997)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][40/390]\tClass 0.0052 (0.1561)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][50/390]\tClass 0.0127 (0.1283)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][60/390]\tClass 0.0077 (0.1113)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][70/390]\tClass 0.0100 (0.0966)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][80/390]\tClass 0.0228 (0.0867)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][90/390]\tClass 0.0003 (0.0779)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][100/390]\tClass 0.0008 (0.0716)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][110/390]\tClass 0.0104 (0.0667)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][120/390]\tClass 0.0000 (0.0627)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][130/390]\tClass 0.0602 (0.0589)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][140/390]\tClass 0.0005 (0.0566)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][150/390]\tClass 0.0000 (0.0541)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][160/390]\tClass 0.0083 (0.0518)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][170/390]\tClass 0.0009 (0.0493)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][180/390]\tClass 0.0000 (0.0471)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][190/390]\tClass 0.0012 (0.0454)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][200/390]\tClass 0.0000 (0.0437)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][210/390]\tClass 0.0322 (0.0422)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][220/390]\tClass 0.0000 (0.0406)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][230/390]\tClass 0.0041 (0.0391)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][240/390]\tClass 0.0002 (0.0378)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][250/390]\tClass 0.0032 (0.0365)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][260/390]\tClass 0.0009 (0.0353)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][270/390]\tClass 0.0373 (0.0342)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][280/390]\tClass 0.0429 (0.0334)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][290/390]\tClass 0.0000 (0.0327)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][300/390]\tClass 0.0058 (0.0320)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][310/390]\tClass 0.0000 (0.0310)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][320/390]\tClass 0.0340 (0.0302)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][330/390]\tClass 0.0000 (0.0294)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][340/390]\tClass 0.0000 (0.0286)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][350/390]\tClass 0.0000 (0.0278)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][360/390]\tClass 0.0011 (0.0270)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][370/390]\tClass 0.0000 (0.0263)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Epoch: [1][380/390]\tClass 0.0000 (0.0259)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
      "INFO:main:Evaluating the model:\n",
      "/home/taehwan/anaconda3/envs/taehwan/lib/python3.7/site-packages/ipykernel_launcher.py:110: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/taehwan/anaconda3/envs/taehwan/lib/python3.7/site-packages/ipykernel_launcher.py:111: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "INFO:main:Test: [0/79]\tClass 32.8866 (32.8866)\tPrec@1 18.750 (18.750)\tPrec@5 57.031 (57.031)\n",
      "INFO:main:Test: [10/79]\tClass 36.3344 (33.7008)\tPrec@1 10.156 (13.565)\tPrec@5 61.719 (57.741)\n",
      "INFO:main:Test: [20/79]\tClass 34.8612 (34.3303)\tPrec@1 9.375 (12.686)\tPrec@5 58.594 (57.217)\n",
      "INFO:main:Test: [30/79]\tClass 30.8027 (34.4230)\tPrec@1 10.156 (12.475)\tPrec@5 59.375 (57.737)\n",
      "INFO:main:Test: [40/79]\tClass 37.1877 (34.5387)\tPrec@1 14.844 (12.538)\tPrec@5 57.812 (58.136)\n",
      "INFO:main:Test: [50/79]\tClass 36.1291 (34.2792)\tPrec@1 10.938 (12.454)\tPrec@5 57.812 (58.747)\n",
      "INFO:main:Test: [60/79]\tClass 39.5975 (34.3489)\tPrec@1 14.844 (12.423)\tPrec@5 60.156 (58.491)\n",
      "INFO:main:Test: [70/79]\tClass 45.6784 (34.2939)\tPrec@1 6.250 (12.566)\tPrec@5 56.250 (58.440)\n",
      "INFO:main: * Prec@1 12.500\tPrec@5 58.450\n"
     ]
    }
   ],
   "source": [
    "epoch = 1\n",
    "\n",
    "train(train_loader, model, optimizer, epoch, training_log)\n",
    "\n",
    "if args.evaluation_epochs and (epoch + 1) % args.evaluation_epochs == 0:\n",
    "    LOG.info(\"Evaluating the model:\")\n",
    "    prec1 = validate(eval_loader, model, validation_log, global_step, epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0043,  0.0101, -0.0080,  ..., -0.0066,  0.0083,  0.0114],\n",
       "        [ 0.0014,  0.0094, -0.0057,  ..., -0.0054,  0.0050,  0.0081],\n",
       "        [-0.0060, -0.0022,  0.0014,  ..., -0.0020, -0.0051, -0.0010],\n",
       "        ...,\n",
       "        [-0.0077, -0.0072,  0.0037,  ...,  0.0017, -0.0087, -0.0048],\n",
       "        [-0.0060, -0.0004, -0.0002,  ..., -0.0019, -0.0045,  0.0003],\n",
       "        [-0.0072, -0.0057,  0.0023,  ..., -0.0013, -0.0072, -0.0046]],\n",
       "       device='cuda:0', grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_preds[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32743,  8250, 31858, 19383, 41196,  7054, 48020,  8557, 34022, 29341,\n",
       "        38399, 22226, 44515, 34395,  3632, 41024, 39514,  1840, 32524, 24091,\n",
       "        47342, 39475, 39291, 18804, 26030, 44980, 20492, 39384, 43416,  2575,\n",
       "         1347, 10859, 11452,  3277, 34050, 47797, 28936, 32067, 32960, 12385,\n",
       "        16871, 33949, 33086, 47683, 26134, 28527,  9977, 25147, 32906, 16865,\n",
       "        42593, 11241, 27625,  7066, 48149,  1760, 21059, 33762, 47180, 31280,\n",
       "        30291, 13692, 26787, 41059, 30957, 46348, 48303, 12305, 35750,  4301,\n",
       "        44994, 26290, 15899,  9853, 24717, 25914, 30066, 47994,  5769, 49577,\n",
       "         1761, 26136,  1102, 37889, 30093,  3354, 16862,  4116,  1435, 44375,\n",
       "        47254, 16819,  7368, 38798, 19646, 39492, 41857, 47842, 49149, 29390,\n",
       "         3890, 19391, 24556,  2670, 45707,   646, 44451, 30504, 22877, 38470,\n",
       "        46557, 39190,  9380,  7325,   656, 25804, 36983, 35469, 14846, 30568,\n",
       "        22274, 17851, 42821, 33971, 37306,  9060, 31706, 10835])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0043,  0.0101, -0.0080,  ..., -0.0066,  0.0083,  0.0114],\n",
       "        [ 0.0014,  0.0094, -0.0057,  ..., -0.0054,  0.0050,  0.0081],\n",
       "        [-0.0060, -0.0022,  0.0014,  ..., -0.0020, -0.0051, -0.0010],\n",
       "        ...,\n",
       "        [-0.0077, -0.0072,  0.0037,  ...,  0.0017, -0.0087, -0.0048],\n",
       "        [-0.0060, -0.0004, -0.0002,  ..., -0.0019, -0.0045,  0.0003],\n",
       "        [-0.0072, -0.0057,  0.0023,  ..., -0.0013, -0.0072, -0.0046]],\n",
       "       device='cuda:0', grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_preds[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter_labels(clean_idxs):\n",
    "    global how_many_labels_filtered\n",
    "\n",
    "    clean_idxs = clean_idxs.tolist()\n",
    "    for i in range(10):\n",
    "        how_many_labels_filtered[i] += clean_idxs.count(i)\n",
    "\n",
    "def filtering(indexes, labels):\n",
    "    global ensemble_preds\n",
    "    global filtered_labels\n",
    "\n",
    "    clean_idxs = np.argwhere(np.argmax(ensemble_preds[indexes].cpu().detach(),axis=1) == labels.cpu())\n",
    "    clean_idxs = clean_idxs[0] # idxs of clean data in minibatch 128\n",
    "    for idx in clean_idxs:\n",
    "        filtered_labels[idx] = 1\n",
    "    return clean_idxs\n",
    "\n",
    "def filtering_(indexes, labels):\n",
    "    global filtered_labels\n",
    "        filtered = list()\n",
    "        for i,component in enumerate(indexes):\n",
    "            if filtered_labels[component] == 1:\n",
    "                filtered.append(indexes[component])\n",
    "        indexes = filtered\n",
    "\n",
    "def update_ensemble_preds(logit1, indexes):\n",
    "    global ensemble_preds\n",
    "    global ema_ensemble_preds\n",
    "\n",
    "    alpha = 0.99\n",
    "    ensemble_preds[indexes] = alpha * ensemble_preds[indexes] + (1-alpha)*logit1\n",
    "\n",
    "def loss_by_filtered_samples(logit1, labels, indexes):\n",
    "    global how_many_labels_filtered\n",
    "\n",
    "    update_ensemble_preds(logit1, indexes)\n",
    "    clean_idx = filtering(indexes, labels)\n",
    "\n",
    "#     logit1_update = logit1[clean_idx]\n",
    "    labels_update = labels[clean_idx]\n",
    "    counter_labels(labels_update)\n",
    "\n",
    "#     size = len(clean_idx) # 0~128\n",
    "    # class_loss\n",
    "#     class_criterion = nn.CrossEntropyLoss(size_average=False).cuda()\n",
    "#     class_loss = class_criterion(logit1_update, labels_update.cuda()) / size\n",
    "\n",
    "    return labels_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 1\n",
      "1 1\n",
      "2 0\n",
      "2 1\n",
      "3 0\n",
      "3 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1, 1]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [0,1,1,0,1,0,0]\n",
    "num = 0\n",
    "for i in range(len(a)):\n",
    "    i -= num\n",
    "    print(i,a[i])\n",
    "    if a[i] == 0:\n",
    "        del a[i]\n",
    "        num += 1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
